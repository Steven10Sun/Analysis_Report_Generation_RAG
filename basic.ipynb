{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steve\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import PyPDF2\n",
    "import os\n",
    "import spacy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from get_embedding_function import get_embedding_function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import transformers\n",
    "summarizer = transformers.pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", revision=\"a4f8f3e\")\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import json\n",
    "from datetime import datetime\n",
    "from library.exportation import export_prompt_response, export_article\n",
    "from questions import questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_chunks(chunks, nlp):\n",
    "    \"\"\"Encode text chunks using spaCy.\"\"\"\n",
    "    encoded_chunks = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        encoded_chunks.append(doc.vector)  # Get the vector representation\n",
    "    return encoded_chunks\n",
    "\n",
    "\n",
    "def summarize_text(text, max_length=100):\n",
    "    summarizer = transformers.pipeline(\"summarization\")\n",
    "    summary = summarizer(text, max_length=max_length, min_length=0, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '6055.HK'\n",
    "folder_name = '3_data'\n",
    "pdf_file_path = os.path.join(folder_name, f'{file_name}.pdf')\n",
    "pdf_text = load_pdf_text(pdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and Embedding for PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True  #add_start_index=True else kernel die\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "text_chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model\n",
    "\n",
    "# Encode the chunks from pdf\n",
    "encoded_chunks = encode_chunks(text_chunks, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 similar token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_response = {}\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    q_emb = nlp(q).vector\n",
    "    # Find the most similar chunks to q_emb\n",
    "    similarities = cosine_similarity([q_emb], encoded_chunks).flatten()\n",
    "\n",
    "    top_3_idx = similarities.argsort()[::-1][:3]\n",
    "    \n",
    "    top_3_text = [text_chunks[i] for i in top_3_idx]\n",
    "\n",
    "    prompt_response[q] = top_3_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(prompt_response):\n",
    "    system_prompt = f\"\"\"\n",
    "    You are an financial analyst for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, say that you don't know. \n",
    "    Use 3 sentences maximum for each question and keep the answer concise.\n",
    "        \\n\\n\n",
    "    Please follow the format to answer the questions.\\n       \n",
    "    \"\"\"\n",
    "\n",
    "    for question, response in prompt_response.items():\n",
    "        response_str = '\\n'.join(response)\n",
    "        system_prompt += f\"Here is the retrieved context:\\n{response_str}\\nQuestion: {question} \\nAnswer: \\n\\n\"\n",
    "    \n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_llm = prompt_generation(prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"1_prompt_log\\\\6055.HK_20241122_1722.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(prompt_to_llm)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put prompt into LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: ChatResponse = chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': prompt_to_llm,\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, I did not find any specific details about the company's approach to diversity and inclusion or the benefits and incentives it offers to retain talent. The text mainly discusses financial matters, such as review procedures, related party transactions, and material agreements.\n",
      "\n",
      "However, as a financial analyst, I can provide some general insights on what companies typically offer to promote diversity and inclusion and retain top talent:\n",
      "\n",
      "* Diversity and Inclusion:\n",
      "\t+ Training programs for employees to recognize and address unconscious biases\n",
      "\t+ Diverse hiring practices, including blind resume reviews or interview panels\n",
      "\t+ Employee resource groups (ERGs) for underrepresented groups to connect and share experiences\n",
      "\t+ Inclusive policies, such as flexible work arrangements or parental leave\n",
      "* Retaining Talent:\n",
      "\t+ Competitive compensation packages, including bonuses and stock options\n",
      "\t+ Professional development opportunities, like mentorship programs or leadership training\n",
      "\t+ Flexible work arrangements, such as telecommuting or compressed workweeks\n",
      "\t+ Recognition and rewards for outstanding performance, like employee of the month/quarter/year awards\n",
      "\n",
      "Please note that these are general suggestions and may not be specific to this company.\n"
     ]
    }
   ],
   "source": [
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = \"\"\"\n",
    "    based on the questions and answers,\\n\n",
    "    generate a analytics report for me.\\n\n",
    "    Please seperate into 5 to 10 paragraphs. Each part should follow the topic below.\\n\n",
    "    1.Company structure and operations.\\n\n",
    "    2.Business segments and their roles.\\n\n",
    "    3.Import/export models and financial performance.\\n\n",
    "    4.Revenue contributions and growth rates.\\n\n",
    "    5.Profit margin analysis and valuation metrics.\"\"\"\n",
    "\n",
    "response2: ChatResponse = chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': summarization_prompt,\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "    return embeddings\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

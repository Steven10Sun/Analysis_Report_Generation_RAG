{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "# import spacy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_chunks(chunks, nlp):\n",
    "    \"\"\"Encode text chunks using spaCy.\"\"\"\n",
    "    encoded_chunks = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        encoded_chunks.append(doc.vector)  # Get the vector representation\n",
    "    return encoded_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pdf_path):\n",
    "    \"\"\"Main function to load PDF, combine text, split it, and encode.\"\"\"\n",
    "    pdf_text = load_pdf_text(pdf_path)\n",
    "\n",
    "    # Initialize the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjust as needed\n",
    "        chunk_overlap=200  # Adjust as needed\n",
    "    )\n",
    "\n",
    "    # Split the text into chunks\n",
    "    text_chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model\n",
    "\n",
    "    # Encode the chunks\n",
    "    encoded_chunks = encode_chunks(text_chunks, nlp)\n",
    "\n",
    "    q = \"what is add/drop period?\"\n",
    "    q_emb = nlp(q).vector\n",
    "\n",
    "    # Find the 5 most similar chunks\n",
    "    similarities = cosine_similarity([q_emb], encoded_chunks).flatten()\n",
    "\n",
    "    # find the top 5\n",
    "    top_5_idx = similarities.argsort()[::-1][:5]\n",
    "\n",
    "    # get the text of the top 5\n",
    "    top_5_text = [text_chunks[i] for i in top_5_idx]\n",
    "\n",
    "    # build the prompt\n",
    "    prompt = \"Please answer the following questions based on the provided text:\\n\\n\"\n",
    "    prompt += f\"Q: {q}\\n\\n\"\n",
    "    prompt += \"Top 5 most relevant text chunks:\\n\\n\"\n",
    "    for i, text in enumerate(top_5_text):\n",
    "        prompt += f\"{i + 1}. {text}\\n\\n\"\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    # # Print the results\n",
    "    # for i, (chunk, encoded) in enumerate(zip(text_chunks, encoded_chunks)):\n",
    "    #     print(f\"Chunk {i + 1}:\\n{chunk}\\n\")\n",
    "    #     print(f\"Encoded Vector {i + 1}:\\n{encoded}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"/Users/cpming/Downloads/Freshmen-Handbook_2022-23_Final.pdf\"  # Replace with your PDF file path\n",
    "    main(pdf_file_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import openai\n",
    "import PyPDF2\n",
    "import spacy\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import transformers\n",
    "# summarizer = transformers.pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", revision=\"a4f8f3e\")\n",
    "from library.exportation import export_prompt_response, export_article\n",
    "from questions import questions\n",
    "from get_embedding_function import get_embedding_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def embedding(chunks, nlp):\n",
    "    \"\"\"Encode text chunks using spaCy.\"\"\"\n",
    "    encoded_chunks = []\n",
    "    for chunk in chunks:\n",
    "        doc = nlp(chunk)\n",
    "        encoded_chunks.append(doc.vector)  # Get the vector representation\n",
    "    return encoded_chunks\n",
    "\n",
    "\n",
    "def summarize_text(text, max_length=100):\n",
    "    summarizer = transformers.pipeline(\"summarization\")\n",
    "    summary = summarizer(text, max_length=max_length, min_length=0, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '6055.HK'\n",
    "folder_name = '3_data'\n",
    "pdf_file_path = os.path.join(folder_name, f'{file_name}.pdf')\n",
    "pdf_text = load_pdf_text(pdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and Embedding for PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True  #add_start_index=True else kernel die\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "text_chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model\n",
    "\n",
    "# Encode the chunks from pdf\n",
    "encoded_chunks = embedding(text_chunks, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 similar token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 3\n",
    "top_N_chunks = {}\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    q_emb = nlp(q).vector\n",
    "    # Find the most similar chunks to q_emb\n",
    "    similarities = cosine_similarity([q_emb], encoded_chunks).flatten()\n",
    "\n",
    "    top_N_idx = similarities.argsort()[::-1][:top_N]\n",
    "    \n",
    "    top_N_text = [text_chunks[i] for i in top_N_idx]\n",
    "\n",
    "    top_N_chunks[q] = top_N_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_prompt(content, question):\n",
    "    system_prompt = f\"\"\"You are an financial analyst.\n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, only say 'I don't know.'. Don't repeat the question.\n",
    "Use 3 sentences maximum for question and keep the answer concise.\n",
    "\n",
    "\n",
    "Retrieved context:\n",
    "{content}\\n\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "    return system_prompt\n",
    "\n",
    "\n",
    "def write_report_prompt(content):\n",
    "    system_prompt = f\"\"\"You are a financial report writer. \n",
    "    Please combine the text provided and generate a financial analysis report. \n",
    "    The report should be about 15 to 20 paragraphs. The report should include the following sections:\n",
    "    1. Company Overview\n",
    "    2. Revenue Structure\n",
    "    3. Profit\n",
    "    4. Valuation\n",
    "    5. Summary\n",
    "    6. Future Outlook\n",
    "    7. and other details provided.\n",
    "    \n",
    "    content is as below:\n",
    "    {content}\n",
    "    \"\"\"\n",
    "def generate_answer(prompt, api_key):\n",
    "    model = 'gpt-4o-mini'\n",
    "    url = \"https://api.ohmygpt.com/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial assistant asking questions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response_json = response.json()\n",
    "    return response_json[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt to llm to ask question\n",
    "all_prompts = []\n",
    "\n",
    "# combine chunks and questions to string\n",
    "for question, top_chunks in top_N_chunks.items():\n",
    "    top_chunks_combined = '.'.join(top_chunks)\n",
    "    system_prompt = ask_question_prompt(top_chunks_combined, question)\n",
    "    all_prompts.append((question, system_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = all_prompts[5:7]\n",
    "sample_prompt_response = []\n",
    "api_key = \"sk-NAWSSGI7999d18B51046T3BlBkFJ514d034054e342cc99c3\"\n",
    "for q, p in sample_prompt:\n",
    "    response = generate_answer(p, api_key)\n",
    "    sample_prompt_response.append((q, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How did the gross profit margin change compared to the previous period?',\n",
       "  \"I don't know.\"),\n",
       " ('What were the operating expenses, and how do they compare to revenue?',\n",
       "  \"I don't know.\")]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prompt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put prompt into LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = \"\"\"\n",
    "    based on the questions and answers,\\n\n",
    "    generate a analytics report for me.\\n\n",
    "    Please seperate into 5 to 10 paragraphs. Each part should follow the topic below.\\n\n",
    "    1.Company structure and operations.\\n\n",
    "    2.Business segments and their roles.\\n\n",
    "    3.Import/export models and financial performance.\\n\n",
    "    4.Revenue contributions and growth rates.\\n\n",
    "    5.Profit margin analysis and valuation metrics.\"\"\"\n",
    "\n",
    "response2: ChatResponse = chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': summarization_prompt,\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        credentials_profile_name=\"default\", region_name=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "    #\n",
    "    # embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "    return embeddings\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
